{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9139f67d",
   "metadata": {},
   "source": [
    "# Part 2: SwiGLU Activation & RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "742bb4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4863d69d",
   "metadata": {},
   "source": [
    "## RMSNorm Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52ca353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return x / rms * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113c8439",
   "metadata": {},
   "source": [
    "## SwiGLU FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5aa4a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff*2, bias=False)    # Gate, Value\n",
    "        self.w2 = nn.Linear(d_ff, d_model, bias=False)    # Output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # gate, value = torch.split(self.w1(x), dim=-1)\n",
    "        gate, value = self.w1(x).chunk(2, dim=-1)\n",
    "        output = self.w2(F.silu(gate) * value)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e044fdca",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e292e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardFFN(nn.Module):\n",
    "    '''FFN with GeLU'''\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff, bias=False)\n",
    "        self.w2 = nn.Linear(d_ff, d_model, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.w2(F.gelu(self.w1(x)))\n",
    "    \n",
    "class CausalSelfAttention(nn.Module):\n",
    "    '''Attention Module'''\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_model // n_head\n",
    "        \n",
    "        self.qkv = nn.Linear(d_model, d_model * 3, bias=False)\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "        \n",
    "        q = q.view(B, T, self.n_head, self.d_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.d_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
    "        attn = attn.masked_fill(torch.tril(torch.ones(T, T)).to(x.device) == 0, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        out = attn @ v\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.proj(out)\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    '''Transformer Block Basic Class'''\n",
    "    def __init__(self, d_model, n_head, d_ff):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.d_ff = d_ff\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class StandardTransformerBlock(TransformerBlock):\n",
    "    '''Config 1: GeLU + Layernorm - nanoGPT'''\n",
    "    def __init__(self, d_model, n_head, d_ff):\n",
    "        super().__init__(d_model, n_head, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_head)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = StandardFFN(d_model, d_ff)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n",
    "    \n",
    "class SwiGLUTransformerBlock(TransformerBlock):\n",
    "    '''Config 2: SwiGLU + RMSNorm - PaLM/Llama'''\n",
    "    def __init__(self, d_model, n_head, d_ff):\n",
    "        super().__init__(d_model, n_head, d_ff)\n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_head)\n",
    "        self.norm2 = RMSNorm(d_model)\n",
    "        self.ffn = SwiGLUFFN(d_model, d_ff)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n",
    "    \n",
    "class HybridTransformerBlock(TransformerBlock):\n",
    "    '''Config 3: SwiGLU + LayerNorm'''\n",
    "    def __init__(self, d_model, n_head, d_ff):\n",
    "        super().__init__(d_model, n_head, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_head)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ffn = SwiGLUFFN(d_model, d_ff)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ffn(self.norm1(x))\n",
    "        return x\n",
    "    \n",
    "class TinyTransformer(nn.Module):\n",
    "    '''Tiny Transformer Module'''\n",
    "    def __init__(self, vocab_size, d_model, n_head, n_layer, d_ff, \n",
    "                 block_type='standard', max_seq_len=1024):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Token & Postion Embeding\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "        # Transformer Block\n",
    "        block_classes = {\n",
    "            'standard': StandardTransformerBlock,\n",
    "            'swiglu': SwiGLUTransformerBlock,\n",
    "            'hybrid': HybridTransformerBlock\n",
    "        }\n",
    "        assert block_type in block_classes, f\"Unknown block type: {block_type}\"\n",
    "        block_class = block_classes[block_type]\n",
    "\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            block_class(d_model, n_head, d_ff) for _ in range(n_layer)\n",
    "        ])\n",
    "\n",
    "        # Norm and Linear Layer\n",
    "        self.norm_f = nn.LayerNorm(d_model) if block_type != 'swiglu' else RMSNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, RMSNorm):\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Tokenizer\n",
    "        token_emb = self.token_emb(idx)     # (B, T, d_model)\n",
    "        pos = torch.arange(T, device=idx.device)\n",
    "        pos_emb = self.pos_emb(pos)         # (T, d_model)\n",
    "        x = token_emb + pos_emb\n",
    "\n",
    "        # Transformer Block\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # Norm & Linear\n",
    "        x = self.norm_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # Loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            inputs = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(inputs, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Truncate to max seq length\n",
    "            idx_cond = idx[:, -self.pos_emb.num_embeddings:]\n",
    "\n",
    "            # Forward\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]/temperature\n",
    "\n",
    "            # Top-K\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)), dim=-1)\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "            # Sample\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02eacff",
   "metadata": {},
   "source": [
    "### Test in Tiny Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "307fa379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 测试 standard 配置 ===\n",
      "输入形状: torch.Size([4, 32])\n",
      "Logits形状: torch.Size([4, 32, 1000])\n",
      "损失值: 6.9787\n",
      "总参数: 3,926,528\n",
      "可训练参数: 3,926,528\n",
      "\n",
      "=== 测试 swiglu 配置 ===\n",
      "输入形状: torch.Size([4, 32])\n",
      "Logits形状: torch.Size([4, 32, 1000])\n",
      "损失值: 6.9252\n",
      "总参数: 4,709,632\n",
      "可训练参数: 4,709,632\n",
      "\n",
      "=== 测试 hybrid 配置 ===\n",
      "输入形状: torch.Size([4, 32])\n",
      "Logits形状: torch.Size([4, 32, 1000])\n",
      "损失值: 6.9384\n",
      "总参数: 4,709,888\n",
      "可训练参数: 4,709,888\n"
     ]
    }
   ],
   "source": [
    "# 测试模型\n",
    "def test_models():\n",
    "    # 参数\n",
    "    vocab_size = 1000\n",
    "    d_model = 256\n",
    "    n_head = 8\n",
    "    n_layer = 6\n",
    "    d_ff = 512\n",
    "    batch_size = 4\n",
    "    seq_len = 32\n",
    "    \n",
    "    # 测试三种配置\n",
    "    configs = ['standard', 'swiglu', 'hybrid']\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\n=== 测试 {config} 配置 ===\")\n",
    "        model = TinyTransformer(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model,\n",
    "            n_head=n_head,\n",
    "            n_layer=n_layer,\n",
    "            d_ff=d_ff,\n",
    "            block_type=config\n",
    "        )\n",
    "        \n",
    "        # 创建随机输入\n",
    "        x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "        \n",
    "        # 前向传播\n",
    "        logits, loss = model(x, x)  # 使用输入作为目标（仅用于测试）\n",
    "        \n",
    "        print(f\"输入形状: {x.shape}\")\n",
    "        print(f\"Logits形状: {logits.shape}\")\n",
    "        print(f\"损失值: {loss.item():.4f}\")\n",
    "        \n",
    "        # 参数计数\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"总参数: {total_params:,}\")\n",
    "        print(f\"可训练参数: {trainable_params:,}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3189fed0",
   "metadata": {},
   "source": [
    "### Training Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8588b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 数据集类（模拟 OpenWebText 子集）\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, num_samples=10000, seq_len=128, vocab_size=1000):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # 生成随机文本数据\n",
    "        self.data = torch.randint(0, vocab_size, (num_samples, seq_len + 1))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx, :-1]\n",
    "        y = self.data[idx, 1:]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# 训练函数\n",
    "def train_model(config_name, block_type, num_steps=10000, \n",
    "                batch_size=32, lr=1e-3, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"训练单个配置的模型\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"开始训练配置: {config_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # 模型参数\n",
    "    vocab_size = 1000\n",
    "    d_model = 256\n",
    "    n_head = 8\n",
    "    n_layer = 6\n",
    "    d_ff = 512\n",
    "    seq_len = 128\n",
    "    \n",
    "    # 创建模型\n",
    "    model = TinyTransformer(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=d_model,\n",
    "        n_head=n_head,\n",
    "        n_layer=n_layer,\n",
    "        d_ff=d_ff,\n",
    "        block_type=block_type,\n",
    "        max_seq_len=seq_len\n",
    "    ).to(device)\n",
    "    \n",
    "    # 创建数据集和DataLoader\n",
    "    dataset = TextDataset(num_samples=5000, seq_len=seq_len, vocab_size=vocab_size)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # 优化器\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # 学习率调度器\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps)\n",
    "    \n",
    "    # 训练记录\n",
    "    losses = []\n",
    "    step_times = []\n",
    "    throughputs = []\n",
    "    \n",
    "    # 训练循环\n",
    "    model.train()\n",
    "    data_iter = iter(dataloader)\n",
    "    global_step = 0\n",
    "    \n",
    "    pbar = tqdm(total=num_steps, desc=f\"训练 {config_name}\")\n",
    "    \n",
    "    while global_step < num_steps:\n",
    "        try:\n",
    "            x, y = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(dataloader)\n",
    "            x, y = next(data_iter)\n",
    "        \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        _, loss = model(x, y)\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # 优化步骤\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 记录时间\n",
    "        step_time = time.time() - start_time\n",
    "        throughput = batch_size / step_time\n",
    "        \n",
    "        # 记录指标\n",
    "        losses.append(loss.item())\n",
    "        step_times.append(step_time)\n",
    "        throughputs.append(throughput)\n",
    "        \n",
    "        # 更新进度条\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'throughput': f'{throughput:.1f} samples/sec'\n",
    "        })\n",
    "        \n",
    "        global_step += 1\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # 计算平均指标\n",
    "    avg_loss = np.mean(losses[-100:])  # 最后100步的平均损失\n",
    "    avg_throughput = np.mean(throughputs[-100:])\n",
    "    \n",
    "    print(f\"\\n训练完成 - {config_name}:\")\n",
    "    print(f\"最终损失: {losses[-1]:.4f}\")\n",
    "    print(f\"平均损失 (最后100步): {avg_loss:.4f}\")\n",
    "    print(f\"平均吞吐量: {avg_throughput:.1f} samples/sec\")\n",
    "    \n",
    "    return {\n",
    "        'config_name': config_name,\n",
    "        'losses': losses,\n",
    "        'throughputs': throughputs,\n",
    "        'step_times': step_times,\n",
    "        'final_loss': losses[-1],\n",
    "        'avg_loss': avg_loss,\n",
    "        'avg_throughput': avg_throughput,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "\n",
    "# 主训练比较函数\n",
    "def run_training_comparison():\n",
    "    # 训练配置\n",
    "    configs = [\n",
    "        ('GeLU+LayerNorm', 'standard'),\n",
    "        ('SwiGLU+RMSNorm', 'swiglu'),\n",
    "        ('SwiGLU+LayerNorm (hybrid)', 'hybrid')\n",
    "    ]\n",
    "    \n",
    "    # 训练参数\n",
    "    num_steps = 10000\n",
    "    batch_size = 32\n",
    "    lr = 1e-3\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    print(f\"使用设备: {device}\")\n",
    "    print(f\"训练步数: {num_steps}\")\n",
    "    print(f\"批次大小: {batch_size}\")\n",
    "    print(f\"学习率: {lr}\")\n",
    "    \n",
    "    # 训练所有配置\n",
    "    results = {}\n",
    "    for config_name, block_type in configs:\n",
    "        result = train_model(\n",
    "            config_name=config_name,\n",
    "            block_type=block_type,\n",
    "            num_steps=num_steps,\n",
    "            batch_size=batch_size,\n",
    "            lr=lr,\n",
    "            device=device\n",
    "        )\n",
    "        results[config_name] = result\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 子图1: 损失曲线\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for config_name in results:\n",
    "        losses = results[config_name]['losses']\n",
    "        # 平滑损失曲线\n",
    "        window = 50\n",
    "        smoothed_losses = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "        steps = np.arange(len(smoothed_losses))\n",
    "        \n",
    "        plt.plot(steps, smoothed_losses, label=config_name, linewidth=2)\n",
    "    \n",
    "    plt.xlabel('训练步数')\n",
    "    plt.ylabel('损失 (平滑)')\n",
    "    plt.title('训练损失曲线 (滑动窗口=50)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 子图2: 吞吐量比较\n",
    "    plt.subplot(1, 2, 2)\n",
    "    config_names = []\n",
    "    avg_throughputs = []\n",
    "    \n",
    "    for config_name in results:\n",
    "        config_names.append(config_name)\n",
    "        avg_throughputs.append(results[config_name]['avg_throughput'])\n",
    "    \n",
    "    bars = plt.bar(config_names, avg_throughputs, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "    plt.xlabel('模型配置')\n",
    "    plt.ylabel('平均吞吐量 (samples/sec)')\n",
    "    plt.title('不同配置的吞吐量比较')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 在柱状图上添加数值标签\n",
    "    for bar, throughput in zip(bars, avg_throughputs):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                f'{throughput:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 打印详细比较结果\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"训练结果总结\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"{'配置':<30} {'最终损失':<12} {'平均损失':<12} {'吞吐量':<12}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for config_name in configs:\n",
    "        config_name = config_name[0]\n",
    "        result = results[config_name]\n",
    "        print(f\"{config_name:<30} {result['final_loss']:<12.4f} {result['avg_loss']:<12.4f} {result['avg_throughput']:<12.1f}\")\n",
    "    \n",
    "    # 分析内存使用\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"内存使用分析\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for config_name in configs:\n",
    "        config_name = config_name[0]\n",
    "        model = results[config_name]['model']\n",
    "        \n",
    "        # 计算参数数量\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        # 估计内存使用\n",
    "        param_memory = total_params * 4 / (1024**2)  # 假设float32，转换为MB\n",
    "        print(f\"{config_name:<30} {total_params:,} 参数 | {param_memory:.1f} MB\")\n",
    "    \n",
    "    # 生成示例文本\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"生成示例文本\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for config_name in configs:\n",
    "        config_name = config_name[0]\n",
    "        model = results[config_name]['model']\n",
    "        \n",
    "        # 设置模型为评估模式\n",
    "        model.eval()\n",
    "        \n",
    "        # 生成起始token\n",
    "        start_tokens = torch.tensor([[1, 2, 3, 4, 5]], device=device)\n",
    "        \n",
    "        # 生成文本\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(start_tokens, max_new_tokens=20, temperature=0.8)\n",
    "        \n",
    "        print(f\"\\n{config_name}:\")\n",
    "        print(f\"输入: {start_tokens[0].cpu().numpy()}\")\n",
    "        print(f\"生成: {generated[0].cpu().numpy()}\")\n",
    "        \n",
    "        # 切换回训练模式\n",
    "        model.train()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# 运行训练比较\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_training_comparison()\n",
    "    \n",
    "    # 保存结果\n",
    "    print(\"\\n保存结果到文件...\")\n",
    "    torch.save(results, 'training_results.pth')\n",
    "    print(\"完成！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ZOO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
