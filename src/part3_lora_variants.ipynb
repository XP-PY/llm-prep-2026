{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e01aa1b",
   "metadata": {},
   "source": [
    "# Part3: LoRA Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d7bc2",
   "metadata": {},
   "source": [
    "## Classic LoRA fine-tune (Transformers Trainer + PEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b6ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B\"  # example; use a model you have access to\n",
    "dataset_id = \"tatsu-lab/alpaca\"       # example dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# LoRA config: pick target_modules for your architecture\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # common for LLaMA-like\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "ds = load_dataset(dataset_id, split=\"train\")\n",
    "\n",
    "def format_example(ex):\n",
    "    # Basic Alpaca-style formatting\n",
    "    inst = ex.get(\"instruction\", \"\")\n",
    "    inp = ex.get(\"input\", \"\")\n",
    "    out = ex.get(\"output\", \"\")\n",
    "    prompt = f\"### Instruction:\\n{inst}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n{out}\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "ds = ds.map(format_example)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "tokenized = ds.map(tokenize, batched=True, remove_columns=ds.column_names)\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./lora_out\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    bf16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./lora_adapter\")\n",
    "tokenizer.save_pretrained(\"./lora_adapter\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf46947",
   "metadata": {},
   "source": [
    "## QLoRA fine-tune (4-bit base + LoRA adapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe9e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B\"  # example\n",
    "dataset_id = \"tatsu-lab/alpaca\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",         # NF4 is a key QLoRA detail\n",
    "    bnb_4bit_use_double_quant=True,    # double quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "ds = load_dataset(dataset_id, split=\"train\")\n",
    "\n",
    "def format_example(ex):\n",
    "    inst = ex.get(\"instruction\", \"\")\n",
    "    inp = ex.get(\"input\", \"\")\n",
    "    out = ex.get(\"output\", \"\")\n",
    "    return {\"text\": f\"### Instruction:\\n{inst}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n{out}\"}\n",
    "\n",
    "ds = ds.map(format_example)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=1024, padding=\"max_length\")\n",
    "\n",
    "tokenized = ds.map(tokenize, batched=True, remove_columns=ds.column_names)\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./qlora_out\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    bf16=True,\n",
    "    optim=\"paged_adamw_8bit\",  # common with bitsandbytes; helps VRAM spikes\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./qlora_adapter\")\n",
    "tokenizer.save_pretrained(\"./qlora_adapter\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1919103a",
   "metadata": {},
   "source": [
    "## DoRA fine-tune (PEFT: use_dora=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeef30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Compare to LoRA setup, DoRA is often just:\n",
    "dora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    use_dora=True,  # <-- DoRA switch exposed by PEFT\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, dora_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e46d4",
   "metadata": {},
   "source": [
    "## LongLoRA with Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d1db88",
   "metadata": {},
   "source": [
    "1. **Practical path A (recommended): use the official LongLoRA repo scripts**\n",
    "\n",
    "The repo is already built around the HF ecosystem and includes:\n",
    "* supervised fine-tuning scripts (and a QLoRA variant)\n",
    "* a script to merge LoRA weights and save a Hugging Face model\n",
    "\n",
    "Example command (structure; exact flags may vary by repo version):\n",
    "  ```bash\n",
    "  git clone https://github.com/JIA-Lab-research/LongLoRA.git\n",
    "  cd LongLoRA\n",
    "  pip install -r requirements.txt\n",
    "  pip install flash-attn --no-build-isolation\n",
    "\n",
    "  # SFT (or use supervised-fine-tune-qlora.py for 4-bit)\n",
    "  torchrun --nproc_per_node 8 supervised-fine-tune.py \\\n",
    "    --base_model path_to/Llama-2-7b-hf \\\n",
    "    --data_path path_to/your_long_sft_data.json \\\n",
    "    --context_size 8192 \\\n",
    "    --output_dir ./longlora_ckpt\n",
    "  ```\n",
    "\n",
    "Then merge adapter weights into a standard HF model using the repo’s merge script (the repo shows a merge command pattern like below).\n",
    "  ```bash\n",
    "  python3 merge_lora_weights_and_save_hf_model.py \\\n",
    "    --base_model path_to/Llama-2-7b-hf \\\n",
    "    --peft_model ./longlora_ckpt \\\n",
    "    --context_size 8192 \\\n",
    "    --save_path ./longlora_merged_hf\n",
    "  ```\n",
    "\n",
    "2. **Practical path B: DIY patch inside Transformers**\n",
    "\n",
    "If you really want everything in one script, the essence is:\n",
    "1. Extend position handling (e.g., RoPE scaling / long-context setup for your model family).\n",
    "2. Swap attention during training to S²-Attn (shifted local attention).\n",
    "3. Apply LoRA/QLoRA with PEFT, plus train embeddings/norms as LongLoRA suggests.\n",
    "\n",
    "But step (2) is the nontrivial bit: you must either import LongLoRA’s attention module or implement the shift-window logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc16d94",
   "metadata": {},
   "source": [
    "## LoHA in Hugging Face (PEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d38b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoHaConfig, get_peft_model, TaskType\n",
    "\n",
    "model_id = \"facebook/opt-125m\"  # replace\n",
    "dataset_id = \"tatsu-lab/alpaca\" # replace\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# LoHA config (note: params differ from LoRA: alpha, rank_dropout, module_dropout)\n",
    "peft_config = LoHaConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    alpha=8,\n",
    "    rank_dropout=0.0,\n",
    "    module_dropout=0.0,\n",
    "    target_modules=\"all-linear\",  # or [\"q_proj\",\"v_proj\",...]\n",
    "    init_weights=True,\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "ds = load_dataset(dataset_id, split=\"train\")\n",
    "\n",
    "def format_ex(ex):\n",
    "    inst, inp, out = ex.get(\"instruction\",\"\"), ex.get(\"input\",\"\"), ex.get(\"output\",\"\")\n",
    "    return {\"text\": f\"### Instruction:\\n{inst}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n{out}\"}\n",
    "\n",
    "ds = ds.map(format_ex)\n",
    "\n",
    "def tok(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=1024, padding=\"max_length\")\n",
    "\n",
    "tokenized = ds.map(tok, batched=True, remove_columns=ds.column_names)\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./loha_out\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "Trainer(model=model, args=args, train_dataset=tokenized, data_collator=collator).train()\n",
    "model.save_pretrained(\"./loha_adapter\")\n",
    "tokenizer.save_pretrained(\"./loha_adapter\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2fc236",
   "metadata": {},
   "source": [
    "## VeRA in Hugging Face (PEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2537236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import VeraConfig, get_peft_model, TaskType\n",
    "\n",
    "model_id = \"facebook/opt-125m\"\n",
    "dataset_id = \"tatsu-lab/alpaca\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "vera_config = VeraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=128,                        # VeRA often uses higher \"r\" than LoRA\n",
    "    target_modules=\"all-linear\",  # VeRA supports nn.Linear (per docs)\n",
    "    vera_dropout=0.0,\n",
    "    projection_prng_key=0,\n",
    "    save_projection=True,         # set False to shrink checkpoints (see docs)\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, vera_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "ds = load_dataset(dataset_id, split=\"train\")\n",
    "\n",
    "def format_ex(ex):\n",
    "    inst, inp, out = ex.get(\"instruction\",\"\"), ex.get(\"input\",\"\"), ex.get(\"output\",\"\")\n",
    "    return {\"text\": f\"### Instruction:\\n{inst}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n{out}\"}\n",
    "\n",
    "ds = ds.map(format_ex)\n",
    "\n",
    "def tok(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=1024, padding=\"max_length\")\n",
    "\n",
    "tokenized = ds.map(tok, batched=True, remove_columns=ds.column_names)\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./vera_out\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "Trainer(model=model, args=args, train_dataset=tokenized, data_collator=collator).train()\n",
    "model.save_pretrained(\"./vera_adapter\")\n",
    "tokenizer.save_pretrained(\"./vera_adapter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ZOO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
