{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e01aa1b",
   "metadata": {},
   "source": [
    "# Part3: LoRA Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d7bc2",
   "metadata": {},
   "source": [
    "## Classic LoRA fine-tune (Transformers Trainer + PEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b6ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B\"  # example; use a model you have access to\n",
    "dataset_id = \"tatsu-lab/alpaca\"       # example dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# LoRA config: pick target_modules for your architecture\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # common for LLaMA-like\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "ds = load_dataset(dataset_id, split=\"train\")\n",
    "\n",
    "def format_example(ex):\n",
    "    # Basic Alpaca-style formatting\n",
    "    inst = ex.get(\"instruction\", \"\")\n",
    "    inp = ex.get(\"input\", \"\")\n",
    "    out = ex.get(\"output\", \"\")\n",
    "    prompt = f\"### Instruction:\\n{inst}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n{out}\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "ds = ds.map(format_example)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "tokenized = ds.map(tokenize, batched=True, remove_columns=ds.column_names)\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./lora_out\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    bf16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./lora_adapter\")\n",
    "tokenizer.save_pretrained(\"./lora_adapter\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf46947",
   "metadata": {},
   "source": [
    "## QLoRA fine-tune (4-bit base + LoRA adapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe9e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B\"  # example\n",
    "dataset_id = \"tatsu-lab/alpaca\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",         # NF4 is a key QLoRA detail\n",
    "    bnb_4bit_use_double_quant=True,    # double quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "ds = load_dataset(dataset_id, split=\"train\")\n",
    "\n",
    "def format_example(ex):\n",
    "    inst = ex.get(\"instruction\", \"\")\n",
    "    inp = ex.get(\"input\", \"\")\n",
    "    out = ex.get(\"output\", \"\")\n",
    "    return {\"text\": f\"### Instruction:\\n{inst}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n{out}\"}\n",
    "\n",
    "ds = ds.map(format_example)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=1024, padding=\"max_length\")\n",
    "\n",
    "tokenized = ds.map(tokenize, batched=True, remove_columns=ds.column_names)\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./qlora_out\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    bf16=True,\n",
    "    optim=\"paged_adamw_8bit\",  # common with bitsandbytes; helps VRAM spikes\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./qlora_adapter\")\n",
    "tokenizer.save_pretrained(\"./qlora_adapter\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1919103a",
   "metadata": {},
   "source": [
    "## DoRA fine-tune (PEFT: use_dora=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeef30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Compare to LoRA setup, DoRA is often just:\n",
    "dora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    use_dora=True,  # <-- DoRA switch exposed by PEFT\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, dora_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ZOO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
