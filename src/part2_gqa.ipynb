{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35e15c0f",
   "metadata": {},
   "source": [
    "# Part 2: Grouped-Query Attention (GQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b374e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class PositionEmbeding(nn.Module):\n",
    "    def __init__(self, max_seq_length):\n",
    "        super().__init__()\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def forward(self, q, k, positions=None):\n",
    "        return q, k\n",
    "    \n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise feed-forward network module\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        # Final output shape: (batch_size, seq_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db082fa",
   "metadata": {},
   "source": [
    "## Multi-Head Attention (MHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d02f3d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention mechanism module with KV Cache support\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Define linear transformation layers for Q, K, V and output\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores (QK^T)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # Apply mask (if provided)\n",
    "        if mask is not None:\n",
    "            # Set positions where mask is 0 to a very small negative number, so they approach 0 after softmax\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Calculate attention weights (Softmax)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Weighted sum (weights * V)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # Transform input x shape from (batch_size, seq_length, d_model)\n",
    "        # to (batch_size, num_heads, seq_length, head_dim)\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # Transform input x shape from (batch_size, num_heads, seq_length, head_dim)\n",
    "        # back to (batch_size, seq_length, d_model)\n",
    "        batch_size, num_heads, seq_length, head_dim = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None,\n",
    "                past_key_value=None, use_cache=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q: Query tensor\n",
    "            K: Key tensor (if None, use Q as K)\n",
    "            V: Value tensor (if None, use Q as V)\n",
    "            mask: Attention mask\n",
    "            past_key_value: Tuple of (past_key, past_value) from previous steps\n",
    "            use_cache: Whether to cache KV for next steps\n",
    "        Returns:\n",
    "            output: Attention output\n",
    "            present_key_value: Tuple of (key, value) for caching (if use_cache=True)\n",
    "        \"\"\"\n",
    "        # Perform linear transformations on Q, K, V\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        # Apply KV Cache if provided\n",
    "        if past_key_value is not None:\n",
    "            # past_key_value is a tuple: (past_key, past_value)\n",
    "            past_key, past_value = past_key_value\n",
    "            # Concatenate past and current KV along sequence dimension\n",
    "            K = torch.cat([past_key, K], dim=2)\n",
    "            V = torch.cat([past_value, V], dim=2)\n",
    "\n",
    "        # Prepare present KV for caching\n",
    "        present_key_value = None\n",
    "        if use_cache:\n",
    "            present_key_value = (K, V)\n",
    "\n",
    "        # Calculate scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine multi-head outputs and perform final linear transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output, present_key_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950c45a8",
   "metadata": {},
   "source": [
    "# Grouped-Query Attention (GQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a2cbcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Grouped query attention mechanism module with KV Cache support.\n",
    "    When num_kv_heads == 1, it euqals Multi-query attention (MQA)\n",
    "    When num_kv_heads == num_heads, it equals Multi-head attention (MHA)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, num_kv_heads):\n",
    "        super(GroupedQueryAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        assert num_heads % num_kv_heads == 0, \"num_heads must be divisible by num_kv_heads\"\n",
    "        self.num_groups = num_heads // num_kv_heads\n",
    "\n",
    "        # Define linear transformation layers for Q, K, V and output\n",
    "        self.W_q = nn.Linear(d_model, num_heads * self.head_dim, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, num_kv_heads * self.head_dim, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, num_kv_heads * self.head_dim, bias=False)\n",
    "        self.W_o = nn.Linear(num_heads * self.head_dim, d_model, bias=False)\n",
    "        \n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores (QK^T)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # Apply mask (if provided)\n",
    "        if mask is not None:\n",
    "            # Set positions where mask is 0 to a very small negative number, so they approach 0 after softmax\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Calculate attention weights (Softmax)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Weighted sum (weights * V)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x, num_heads):\n",
    "        '''Transform input x shape from (batch_size, seq_length, d_model)\n",
    "        to (batch_size, num_heads, seq_length, head_dim)'''\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        '''Transform input x shape from (batch_size, num_heads, seq_length, head_dim)\n",
    "        back to (batch_size, seq_length, d_model)'''\n",
    "        batch_size, num_heads, seq_length, head_dim = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "    \n",
    "    def expand_kv(self, K, V):\n",
    "        '''Repeat KV for groups (before caching for better efficiency)'''\n",
    "        # K, V shape: (batch_size, num_kv_heads, seq_length, head_dim)\n",
    "        if self.num_groups > 1:\n",
    "            # Method 1: Repeat along head dimension\n",
    "            K = K.repeat_interleave(self.num_groups, dim=1)\n",
    "            V = V.repeat_interleave(self.num_groups, dim=1)\n",
    "            \n",
    "            # Alternative method 2: Expand (less memory efficient)\n",
    "            # B, _, T, _ = K.shape\n",
    "            # K = K.unsqueeze(2).expand(-1, -1, self.num_groups, -1, -1).reshape(B, self.num_heads, T, self.head_dim)\n",
    "            # V = V.unsqueeze(2).expand(-1, -1, self.num_groups, -1, -1).reshape(B, self.num_heads, T, self.head_dim)\n",
    "        return K, V\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None,\n",
    "                past_key_value=None, use_cache=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q: Query tensor of shape (batch_size, seq_len, d_model)\n",
    "            K: Key tensor (optional)\n",
    "            V: Value tensor (optional)\n",
    "            mask: Attention mask\n",
    "            past_key_value: Tuple of (past_key, past_value) from previous steps\n",
    "            use_cache: Whether to cache KV for next steps\n",
    "        Returns:\n",
    "            output: Attention output\n",
    "            present_key_value: Tuple of (key, value) for caching\n",
    "        \"\"\"\n",
    "        Q = self.split_heads(self.W_q(Q), self.num_heads)       # (batch_size, num_heads, seq_len, head_dim)\n",
    "        K = self.split_heads(self.W_k(K), self.num_kv_heads)    # (batch_size, num_kv_heads, seq_len, head_dim)\n",
    "        V = self.split_heads(self.W_v(V), self.num_kv_heads)    # (batch_size, num_kv_heads, seq_len, head_dim)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            past_key, past_value = past_key_value\n",
    "            K = torch.cat([past_key, K], dim=2)\n",
    "            V = torch.cat([past_value, V], dim=2)\n",
    "\n",
    "        present_key_value = None\n",
    "        if use_cache:\n",
    "            present_key_value = (K, V)\n",
    "\n",
    "        K, V = self.expand_kv(K, V)\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output, present_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f9e6705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape without cache: torch.Size([2, 10, 512])\n",
      "Cache keys shape: torch.Size([2, 2, 10, 64])\n",
      "Output shape with cache: torch.Size([2, 1, 512])\n",
      "New cache keys shape: torch.Size([2, 2, 11, 64])\n",
      "\n",
      "KV sharing ratio: 8:2 = 4x\n",
      "Memory savings: 75.0%\n"
     ]
    }
   ],
   "source": [
    "def test_gqa():\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    num_kv_heads = 2  # GQA: 8 query heads share 2 KV heads\n",
    "    \n",
    "    gqa = GroupedQueryAttention(d_model, num_heads, num_kv_heads)\n",
    "    \n",
    "    # Test inputs\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Test without cache\n",
    "    output1, cache = gqa(x, x, x, use_cache=True)\n",
    "    print(f\"Output shape without cache: {output1.shape}\")\n",
    "    print(f\"Cache keys shape: {cache[0].shape}\")  # Should be (2, 2, 10, 64)\n",
    "    \n",
    "    # Test with cache (autoregressive step)\n",
    "    next_token = torch.randn(batch_size, 1, d_model)\n",
    "    output2, new_cache = gqa(\n",
    "        next_token, next_token, next_token,\n",
    "        past_key_value=cache,\n",
    "        use_cache=True\n",
    "    )\n",
    "    print(f\"Output shape with cache: {output2.shape}\")\n",
    "    print(f\"New cache keys shape: {new_cache[0].shape}\")  # Should be (2, 2, 11, 64)\n",
    "    \n",
    "    # Verify KV sharing\n",
    "    print(f\"\\nKV sharing ratio: {num_heads}:{num_kv_heads} = {num_heads/num_kv_heads:.0f}x\")\n",
    "    print(f\"Memory savings: {(1 - num_kv_heads/num_heads)*100:.1f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_gqa()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ZOO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
