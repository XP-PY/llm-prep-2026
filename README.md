# Part 1: Solidify foundations.

| Milestone                         | Status    | Notes               |
|:------------------------------------:|:-----------:|:---------------------:|
| Repo Created & Initial Commit     | Complete  | https://github.com/XP-PY/llm-prep-2026 |
| [SVD + Attention](./docs/Attention_Machanisms/SVD_Attention.md) | Complete  | Notebook + Code |
| [AdamW](./docs/Optimizer/AdamW.md) | Complete  | Notebook + Code |
| [RoPE](./docs/Position_Embeding/RoPE.md) | Complete  | Notebook + Code |

# Part 2: Master the key innovations that make modern LLMs fast and memory-efficient at scale.
| Milestone                         | Status    | Notes               |
|:------------------------------------:|:-----------:|:---------------------:|
| [FlashAttention](./docs/Attention_Machanisms/FlashAttention.md) | Cpmplete  | Notebook |
| [GQA/MQA](./docs/Attention_Machanisms/GQA.md) | Complete  | Notebook + Code |
| [SwiGLU & RMSNorm](./docs/Activation_Layers/SwiGLU.md) | Complete  | Notebook + Code |